import OpenAI from "openai";
import weaviate from "weaviate-ts-client";
import dotenv from "dotenv";
import fs from "fs";
import { fkSystemPrompt } from "./prompts/fkSystemPrompt.js";
dotenv.config();

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const client = weaviate.client({
  scheme: "https",
  host: process.env.WEAVIATE_HOST,
  apiKey: new weaviate.ApiKey(process.env.WEAVIATE_API_KEY),
});

const indexName = "FK_Document";

// Moderna modellnamn (kan overrideas via .env)
const CHAT_MODEL = process.env.OPENAI_CHAT_MODEL || "gpt-4o-mini";
const EMBEDDING_MODEL = process.env.OPENAI_EMBED_MODEL || "text-embedding-3-small";

// Toggle: s√§tt RAG_DISABLED=true i .env f√∂r att hoppa √∂ver Weaviate helt
const RAG_DISABLED = String(process.env.RAG_DISABLED).toLowerCase() === "true";

/* -------------------- H√§lsning/ack/avslut (sm√•prat) -------------------- */
const GREETING_RESPONSE =
  "Hej. Du chattar med F√∂rs√§kringskassans chattbot. Hur kan jag hj√§lpa dig?";
const THANKS_RESPONSE =
  "Vars√•god! Beh√∂ver du mer hj√§lp √§r du v√§lkommen att st√§lla en ny fr√•ga.";
const GOODBYE_RESPONSE =
  "Tack! Ha en fortsatt bra dag. V√§lkommen tillbaka om du undrar n√•got mer.";

function norm(s) {
  return (s || "")
    .toLowerCase()
    .replace(/[!?.‚Ä¶,:;()"'`~]/g, "")
    .replace(/\s+/g, " ")
    .trim();
}

// Klassificera sm√•prat: greeting | thanks | goodbye | null
function classifySmalltalk(input) {
  const t = norm(input);
  if (!t) return "greeting";
  const isGreeting = [
    "hej", "hej hej", "hejsan", "tja", "tjabba", "tjena", "hall√•",
    "god morgon", "god kv√§ll", "hello", "hi", "hey",
  ].some((g) => t === g);
  if (isGreeting) return "greeting";
  if (/\b(tack|tackar|tusen tack|stort tack|tack s√• mycket)\b/.test(t)) return "thanks";
  if (/\b(ok|okej|okey)\b/.test(t) && t.length <= 20) return "thanks";
  if (/\b(hej d√•|hejd√•|vi h√∂rs|ha det|trevlig dag|adj√∂|bye|p√• √•terseende)\b/.test(t)) return "goodbye";
  return null;
}
/* ---------------------------------------------------------------------- */

/* --------- Rensa ev. k√§llrader om modellen l√§gger till dem ----------- */
function cleanAnswer(text = "") {
  // Ta bort rader som b√∂rjar med "K√§lla:" eller "K√§llor:" (oavsett versaler/√•/√§)
  const withoutSource = text.replace(/^\s*K(?:√§|a)ll(?:a|or)\s*:\s.*$/gmi, "").trim();
  return withoutSource.replace(/\n{3,}/g, "\n\n").trim();
}
/* ---------------------------------------------------------------------- */

/* ------------ Kondensera f√∂ljdfr√•ga ‚Üí frist√•ende fr√•ga ------------- */
const MAX_HISTORY = 6; // senaste 6 meddelanden r√§cker l√•ngt

function sanitizeHistory(history = []) {
  return history
    .filter(
      (m) =>
        m &&
        (m.role === "user" || m.role === "assistant") &&
        typeof m.content === "string" &&
        m.content.trim().length > 0
    )
    .slice(-MAX_HISTORY);
}

async function condenseQuestion(history, latestUserInput) {
  const safeHist = sanitizeHistory(history);
  if (safeHist.length === 0) return (latestUserInput || "").trim();

  const convo = safeHist
    .map((m) => `${m.role === "assistant" ? "Bot" : "Anv√§ndare"}: ${m.content}`)
    .join("\n");

  const sys =
    "Du skriver om anv√§ndarens senaste inmatning till en FRIST√ÖENDE, tydlig fr√•ga om F√∂rs√§kringskassan. " +
    "Anv√§nd endast uppgifter som redan finns i samtalet. Ingen extra text, inga citattecken ‚Äì returnera bara den omskrivna fr√•gan p√• svenska.";

  const user = `Samtal hittills:\n${convo}\n\nSenaste inmatning: "${latestUserInput}"\n\nSkriv nu en frist√•ende fr√•ga:`;

  const resp = await openai.chat.completions.create({
    model: CHAT_MODEL,
    messages: [
      { role: "system", content: sys },
      { role: "user", content: user },
    ],
    temperature: 0.1,
  });

  const q = resp.choices?.[0]?.message?.content?.trim();
  return q && q.length > 0 ? q : (latestUserInput || "").trim();
}
/* ------------------------------------------------------------------ */

export async function askRAG(query, history = []) {
  // 0) Sm√•prat f√∂rst ‚Äì ta det innan RAG/fallback
  const st = classifySmalltalk(query);
  if (st === "greeting") return GREETING_RESPONSE;
  if (st === "thanks") return THANKS_RESPONSE;
  if (st === "goodbye") return GOODBYE_RESPONSE;

  // 1) G√∂r senaste inmatning till frist√•ende fr√•ga baserat p√• historiken
  const standaloneQuestion = await condenseQuestion(history, query);

  // 2) Snabbt demo-l√§ge utan RAG
  if (RAG_DISABLED) {
    console.warn("RAG_DISABLED=true ‚Äî hoppar √∂ver vektors√∂k och anv√§nder GPT direkt.");
    return await fallbackToGPT(standaloneQuestion);
  }

  try {
    // 3) Embedding p√• den frist√•ende fr√•gan
    console.log("üîç Skickar fr√•ga till OpenAI f√∂r embedding (kondenserad)...");
    const embeddingResponse = await openai.embeddings.create({
      model: EMBEDDING_MODEL,
      input: standaloneQuestion,
    });
    const queryEmbedding = embeddingResponse.data[0].embedding;

    // 4) Vektors√∂k
    console.log("üß† Fr√•gar Weaviate med vektor...");
    let result;
    try {
      result = await client.graphql
        .get()
        .withClassName(indexName)
        .withFields("text")
        .withNearVector({ vector: queryEmbedding })
        .withLimit(5)
        .do();
    } catch (weavErr) {
      console.error("‚ùå Weaviate-fel:", weavErr?.message || weavErr);
      return await fallbackToGPT(standaloneQuestion);
    }

    const docs = result?.data?.Get?.[indexName] || [];
    if (docs.length === 0) {
      console.warn("‚ö†Ô∏è Inga tr√§ffar i Weaviate, anv√§nder fallback till GPT direkt...");
      return await fallbackToGPT(standaloneQuestion);
    }

    // 5) Bygg RAG-kontekst
    const context = docs.map((doc) => doc.text).join("\n---\n");

    const prompt = `
Du √§r en hj√§lpsam AI-assistent som svarar med korrekt information fr√•n F√∂rs√§kringskassan.
Anv√§nd bara fakta fr√•n TEXT nedan n√§r du besvarar fr√•gan. Skriv svaret utan k√§llh√§nvisning.

TEXT:
${context}

FR√ÖGA: ${standaloneQuestion}
SVAR:
    `.trim();

    // 6) Svara, med FK-systemprompten
    console.log("üí¨ Skickar prompt till GPT (med systemprompt)...");
    const chatResponse = await openai.chat.completions.create({
      model: CHAT_MODEL,
      messages: [
        { role: "system", content: fkSystemPrompt },
        { role: "user", content: prompt },
      ],
      temperature: 0.1,
    });

    const raw = chatResponse.choices?.[0]?.message?.content || "";
    return cleanAnswer(raw) || "Jag vet tyv√§rr inte.";
  } catch (error) {
    console.error(
      "‚ùå Fel i RAG-s√∂kning:",
      error?.response?.data?.error?.message || error.message
    );
    return await fallbackToGPT(standaloneQuestion);
  }
}

async function fallbackToGPT(standaloneQuestion) {
  try {
    const fallbackPrompt = `
Besvara endast fr√•gor som r√∂r F√∂rs√§kringskassan. Om underlaget √§r oklart, st√§ll EN precis f√∂ljdfr√•ga.
Skriv svaret utan k√§llh√§nvisning.
FR√ÖGA: ${standaloneQuestion}
SVAR:
    `.trim();

    const chatResponse = await openai.chat.completions.create({
      model: CHAT_MODEL,
      messages: [
        { role: "system", content: fkSystemPrompt },
        { role: "user", content: fallbackPrompt },
      ],
      temperature: 0.1,
    });

    const raw = chatResponse.choices?.[0]?.message?.content || "";
    return cleanAnswer(raw) || "Jag vet tyv√§rr inte.";
  } catch (error) {
    console.error(
      "‚ùå Fel i fallback till GPT:",
      error?.response?.data?.error?.message || error.message
    );
    return "Ett fel uppstod vid fallback-svar fr√•n GPT.";
  }
}
